---

### Ansible playbook for immutable KOps in multi-account / region combos
### Contributors : Andrew J Krug @andrewkrug irc: andrew
### This is licensed under the Mozilla Public License 2.0

- name: provision or update a stack
  hosts: localhost
  connection: local
  gather_facts: false

  vars:
      ### Fetch the important stuff from the users / CI Environment.
      ### At least it is not credentials, right?
      AWS_REGION: "{{ lookup('env', 'AWS_REGION') }}"
      STAGE: "{{ lookup('env', 'STAGE') }}"


  tasks:
    - name: Remove temp files
      shell: find temp -type f -not -name '.keep' -delete

    - name: Generate codename
      script: ./generate-codename.py
      register: codename

    - name: clustername not supplied
      debug: msg="clustername not supplied setting fact of clustername {{codename.stdout}}"
      when: clustername is not defined

    - name: find aws account id
      shell: aws sts get-caller-identity|jq -r ".Account"
      register: account_id

    - name: set clustername
      set_fact: clustername="{{ codename.stdout }}.{{domain}}"
      when: clustername is not defined

    - name: attempting to locate cluster
      debug: msg="attempting to locate cluster {{clustername}}"

    ### Query the AWS account you are assumed role in.  Determine whether the
    ### cluster exists by searching for running instances.
    - ec2_instance_facts:
        region: "{{AWS_REGION}}"
        filters:
          "instance-state-name": "running"
          "tag:KubernetesCluster": "{{clustername}}"
      register: current_instances

    ### Report the number of instances found by the search.
    - name: instances-found
      debug: msg="Located instances matching tag {{ current_instances['instances']|length }}."

    ### Render a value template
    - template:
        src: "../kops-configs/values/{{STAGE}}/{{AWS_REGION}}.yml.j2"
        dest: "../kops-configs/values/{{STAGE}}/{{AWS_REGION}}.yml"

    ### Render a new template from the configuration regardless of state.
    - name: render a new template
      vars:
        kops_values: "values/{{STAGE}}/{{AWS_REGION}}.yml"
        kops_template: "cluster-1.0.yml"
        kops_output: "rendered-templates/{{STAGE}}-{{AWS_REGION}}.yml"
      shell: kops toolbox template {{clustername}} --values {{kops_values}} --template {{kops_template}} > {{kops_output}}
      args:
        chdir: ../kops-configs/

    ### If the cluster exists replace the existing yml configuration in the state store.
    - name: update cluster using the new template
      vars:
        kops_output: "rendered-templates/{{STAGE}}-{{AWS_REGION}}.yml"
      shell: kops replace -f {{kops_output}}
      args:
        chdir: ../kops-configs/
      when: current_instances['instances']|length > 0

    ### If the state store is updated then update the cluster.
    - name: trigger the proposal to update nodes in cluster
      shell: kops update cluster {{clustername}} --yes
      args:
        chdir: ../kops-configs/
      when: current_instances['instances']|length > 0

    ### Trigger a rolling update to all nodes.  This will briefly cycle nodes out.
    ### Hopefully your applications are as HA as your cluster when this runs ;)
    - name: rolling update the cluster
      vars:
        kops_output: "rendered-templates/{{STAGE}}-{{AWS_REGION}}.yml"
      shell: kops update cluster {{clustername}} --yes
      args:
        chdir: ../kops-configs/
      when: current_instances['instances']|length > 0

    ### If the cluster does not exist yet create it.
    - name: create the cluster
      vars:
        kops_output: "rendered-templates/{{STAGE}}-{{AWS_REGION}}.yml"
      shell: kops create -f {{kops_output}}
      args:
        chdir: ../kops-configs/
      when: current_instances['instances']|length == 0

    ### Import a default keypair for node ssh access.
    - name: generate a keypair for the cluster
      shell: ssh-keygen -t rsa -f temp/{{clustername}} -N ''
      when: current_instances['instances']|length == 0

    ### Import a default keypair for node ssh access.
    - name: import and associate the preferred keypair
      shell: kops create secret --name {{clustername}} sshpublickey admin -i temp/{{clustername}}.pub
      when: current_instances['instances']|length == 0

    ### Actually run the KOps provisioner to build ASGs, instances, etc.
    - name: provision the nodes and masters
      shell: kops update cluster {{clustername}} --yes
      args:
        chdir: ../kops-configs/
      when: current_instances['instances']|length == 0

    - template:
        src: files/kops-master-iam-policy.json.j2
        dest: temp/kops-master-iam-policy.json

    # Replace the Master Role with our customized one.  This is an inline policy.
    - name: update master IAM role with custom hardened role
      iam_policy:
        region: "{{AWS_REGION}}"
        iam_type: role
        iam_name: masters.{{clustername}}
        policy_name: masters.{{clustername}}
        state: present
        policy_document: temp/kops-master-iam-policy.json

    - template:
        src: files/kops-nodes-iam-policy.json.j2
        dest: temp/kops-nodes-iam-policy.json

    # Replace the Node Role with our customized one.  This is an inline policy.
    - name: update node IAM role with custom hardened role
      iam_policy:
        region: "{{AWS_REGION}}"
        iam_type: role
        iam_name: nodes.{{clustername}}
        policy_name: nodes.{{clustername}}
        state: present
        policy_document: temp/kops-nodes-iam-policy.json

    - template:
        src: files/kubernetes-assumable-roles.json.j2
        dest: temp/kubernetes-assumable-roles.json

    ### Add a policy to the node role that allows the nodes to assumeRole
    ### for a whitelist of roles.
    - name: update node role with whitelist of assumable roles
      iam_policy:
        region: "{{AWS_REGION}}"
        iam_type: role
        iam_name: nodes.{{clustername}}
        policy_name: assumable-roles
        state: present
        policy_document: temp/kubernetes-assumable-roles.json

    ### We use SSM to admin the cluster so we need to
    ### Also add the trust policy for ssm.*
    - name: add SSM support for nodes
      iam_role:
        name: nodes.{{clustername}}
        assume_role_policy_document: "{{ lookup('file','trust-policy.json') }}"
        state: present

    ### We use SSM to admin the cluster so we need to
    ### Also add the trust policy for ssm.*
    - name: add SSM support for masters
      iam_role:
        name: masters.{{clustername}}
        assume_role_policy_document: "{{ lookup('file','trust-policy.json') }}"
        state: present

    - template:
        src: files/kubernetes-ssm-execution.json.j2
        dest: temp/kubernetes-ssm-execution.json

    ### Add a policy to the node role that allows ssm
    - name: update masters role with ssm permissions
      iam_policy:
        region: "{{AWS_REGION}}"
        iam_type: role
        iam_name: masters.{{clustername}}
        policy_name: ssm
        state: present
        policy_document: temp/kubernetes-ssm-execution.json

    ### Add a policy to the node role that allows ssm

    - name: update node role with ssm permissions
      iam_policy:
        region: "{{AWS_REGION}}"
        iam_type: role
        iam_name: nodes.{{clustername}}
        policy_name: ssm
        state: present
        policy_document: temp/kubernetes-ssm-execution.json

    ### Create the ingress controller role
    - name: Create IAM role for the ingress controller service
      iam:
        iam_type: role
        name: alb-ingress.{{clustername}}
        state: present
        path: /kubernetes/assumeable/
        trust_policy:
          Version: '2012-10-17'
          Statement:
          - Action: sts:AssumeRole
            Effect: Allow
            Principal:
              AWS: arn:aws:iam::{{account_id.stdout}}:role/nodes.{{clustername}}
          - Action: sts:AssumeRole
            Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
      register: alb_role

    - name: Alb role arn notification.
      debug: msg="{{alb_role['role_result']['arn']}}"

    - template:
        src: files/alb-ingress.json.j2
        dest: temp/alb-ingress.json

    - name: attach policy to the ingress controller role
      iam_policy:
        region: "{{AWS_REGION}}"
        iam_type: role
        iam_name: alb-ingress.{{clustername}}
        policy_name: ingress-permissions
        state: present
        policy_document: temp/alb-ingress.json

    ### Build a list of anything YAML
    - name: Enumerate configurations to deploy
      find:
        paths: ../kubernetes/
        patterns: '*.yml,*.yaml'
        recurse: yes
      register: deploy_yaml

    ### Run a KOps Validation to determine that the cluster is healthy.
    - name: validate the cluster after the update
      shell: kops validate cluster {{clustername}}
      args:
        chdir: ../kops-configs/
      retries: 10
      delay: 30
      when: current_instances['instances']|length > 0

    - name: tell the user this is the first time this cluster was created.
      debug: msg="This is the first time this cluster has come up.  Wait 5-10 minutes and then run kops validate cluster {{clustername}}"
      when: current_instances['instances']|length == 0

    - name: tell the user to add the role to trust relationships.
      debug: msg="Dont forget to add our node role to all the trust relationships of any other roles it may need to assume."
      when: current_instances['instances']|length == 0

    ### Fetch the cluster admin creds and use that context to bootstrap
    - name: fetch creds and switch context
      shell: kops export kubecfg
      when: current_instances['instances']|length > 0

    - template:
        src: ../kubernetes/services/alb-ingress-controller.yaml.j2
        dest: ../kubernetes/services/alb-ingress-controller.yaml

    ### Deploy each yaml regardless of what it is.
    - name: deploy all the things
      shell: kubectl apply -f {{ item['path']}}
      with_items: "{{ deploy_yaml['files'] }}"
      when: current_instances['instances']|length > 0
